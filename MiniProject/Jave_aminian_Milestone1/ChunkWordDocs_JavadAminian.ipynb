{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NqMv3iTNUGM",
    "outputId": "c89f85bd-2769-4e9e-b710-65f1bb8558bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /new/benpyenv/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: docx in /new/benpyenv/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: lxml in /new/benpyenv/lib/python3.10/site-packages (from docx) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /new/benpyenv/lib/python3.10/site-packages (from docx) (10.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "!pip install python-docx\n",
    "!pip install docx\n",
    "\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "# from google.colab import files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YzICc7GNDQS"
   },
   "source": [
    "# RAG Mini Project\n",
    "## Milestone #1 : Create and store Chunks\n",
    "This notebook shows how to create text chunks from MS Word Documents.  \n",
    "- Chunks all the word documents in a directory\n",
    "- Uses python-docs to extract paragraph text for chunking\n",
    "- Paragraphs are merged depending on parameterizable  max chunk size\n",
    "- Document cleaning recommended for best results\n",
    "-- remove diagrams and unnecessary text\n",
    "-- merge paragraphs that are semantically similar\n",
    "\n",
    "## Deliverables:\n",
    "- Selection of multiple documents for your RAG project\n",
    "- Capture chunks in a pickle file for next step (Embeddings)\n",
    "\n",
    "When you generate the chunks with the size and semantics you want, then store them into a List and use Python's pickle library to save any Python data structure in a pickle file (.pkl) for later use and then recreate the data structure by loading the pickle file.\n",
    "\n",
    "Deliverables:\n",
    "\n",
    "- List of documents for your RAG project\n",
    "- Jupyter Notebook\n",
    "- Short summary of your efforts (issues, successes...)\n",
    "- Pickle file with chunks for next step (Milestone 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How to use pickle (good for Python data)\n",
    "<code>import pickle<br>\n",
    "my_list = ['apple', 'banana', 42, [1, 2, 3]]</code>\n",
    "\n",
    "### Save the list\n",
    "<code>with open('mylist.pkl', 'wb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; pickle.dump(my_list, f)\n",
    "</code>\n",
    "### Load the list\n",
    "<code>with open('mylist.pkl', 'rb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; loaded_list = pickle.load(f)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4jvsOqwMpQa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/javad/Downloads/INFO290/content/1.BiasInMachineLearning.docx',\n",
       " '/home/javad/Downloads/INFO290/content/2.BiasInMachineLearning_CaseStudies.docx',\n",
       " '/home/javad/Downloads/INFO290/content/3.RegulatoryApproaches_AI_Bias.docx',\n",
       " '/home/javad/Downloads/INFO290/content/4.FairnessTechniquesInML.docx',\n",
       " '/home/javad/Downloads/INFO290/content/5.Perspectives_EthicalAI.docx']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "\n",
    "# Define the directory where files will be saved\n",
    "save_directory = \"/home/javad/Downloads/INFO290/content\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Creating a new set of documents on Bias in Machine Learning with multiple paragraphs\n",
    "documents = {\n",
    "    \"1.BiasInMachineLearning.docx\": [\n",
    "        \"Bias in machine learning (ML) has evolved alongside AI itself, reflecting the challenges of ensuring fairness in automated decision-making. \"\n",
    "        \"Early AI systems inherited human biases due to limited representative data. This document traces the historical development of bias in AI systems, \"\n",
    "        \"from early statistical models to deep learning and modern AI ethics concerns.\",\n",
    "        \n",
    "        \"Early statistical models (1950s-1980s) relied on manually curated datasets, often reflecting biases in data collection. \"\n",
    "        \"Linear regression and early statistical learning approaches exhibited bias due to unrepresentative data.\",\n",
    "        \n",
    "        \"Machine learning growth (1990s-2010s) introduced more complex models, increasing reliance on historical data. \"\n",
    "        \"Neural networks and decision trees amplified existing biases, leading to issues in credit scoring and facial recognition.\",\n",
    "        \n",
    "        \"The deep learning era (2010s-present) introduced large-scale AI models like GPT and DALL·E, demonstrating biases in language and image generation. \"\n",
    "        \"Ethical concerns prompted fairness-aware algorithms and regulatory responses.\"\n",
    "    ],\n",
    "\n",
    "    \"2.BiasInMachineLearning_CaseStudies.docx\": [\n",
    "        \"AI bias has resulted in real-world consequences across multiple sectors. This document analyzes major case studies demonstrating the effects of bias in machine learning.\",\n",
    "        \n",
    "        \"Hiring and employment: Amazon's hiring algorithm (2018) showed gender bias, downgrading female candidates due to past hiring patterns. \"\n",
    "        \"This case demonstrated how ML models can reinforce historical inequalities if trained on biased datasets.\",\n",
    "        \n",
    "        \"Law enforcement and criminal justice: The COMPAS algorithm used for criminal sentencing showed racial bias, falsely labeling Black defendants \"\n",
    "        \"as high risk at twice the rate of white defendants. This sparked concerns over AI-driven legal discrimination.\",\n",
    "        \n",
    "        \"Healthcare disparities: A healthcare risk-prediction algorithm exhibited racial bias, allocating fewer resources to Black patients with similar \"\n",
    "        \"health conditions as white patients. This highlights the risk of biased AI in critical decision-making areas.\",\n",
    "        \n",
    "        \"Financial sector: AI-driven credit scoring systems demonstrated bias against minority communities, leading to disproportionate loan denials. \"\n",
    "        \"Such biases underscore the need for transparency in AI-based financial decisions.\"\n",
    "    ],\n",
    "\n",
    "    \"3.RegulatoryApproaches_AI_Bias.docx\": [\n",
    "        \"Governments worldwide are taking different approaches to regulate and mitigate bias in AI. This document explores key regulatory efforts.\",\n",
    "        \n",
    "        \"The European Union has introduced the AI Act (2021), which classifies AI systems based on risk levels. \"\n",
    "        \"High-risk systems must meet strict fairness and transparency requirements.\",\n",
    "        \n",
    "        \"The United States lacks a comprehensive AI bias law but relies on the Federal Trade Commission (FTC) to enforce fairness \"\n",
    "        \"under consumer protection laws. States like Illinois have passed biometric data regulations.\",\n",
    "        \n",
    "        \"China has implemented AI governance guidelines emphasizing fairness and transparency. Companies deploying AI must ensure non-discrimination in algorithms.\",\n",
    "        \n",
    "        \"Global organizations like UNESCO and OECD advocate for ethical AI principles focusing on non-discrimination, transparency, and fairness.\"\n",
    "    ],\n",
    "\n",
    "    \"4.FairnessTechniquesInML.docx\": [\n",
    "        \"Researchers have developed multiple techniques to reduce AI bias. This document outlines fairness-aware machine learning approaches.\",\n",
    "        \n",
    "        \"Pre-processing methods focus on data balancing techniques such as re-weighting samples and synthetic data generation to improve fairness before training.\",\n",
    "        \n",
    "        \"In-processing techniques include adversarial debiasing, where models are trained to minimize sensitive attribute influence, and regularization techniques \"\n",
    "        \"that encourage fair decision boundaries.\",\n",
    "        \n",
    "        \"Post-processing techniques involve model auditing and explainability tools such as SHAP and LIME, which help interpret biased outcomes. \"\n",
    "        \"Adjusting model predictions to meet fairness constraints is another key approach.\",\n",
    "        \n",
    "        \"Fairness-aware algorithms continue to evolve, integrating ethical considerations into AI development to mitigate bias and improve inclusivity.\"\n",
    "    ],\n",
    "\n",
    "    \"5.Perspectives_EthicalAI.docx\": [\n",
    "        \"AI fairness and ethics are global concerns. This document examines different international approaches to bias mitigation in AI.\",\n",
    "        \n",
    "        \"The United Nations advocates for human rights-based AI governance, emphasizing the importance of fairness and transparency in automated decision-making.\",\n",
    "        \n",
    "        \"The European Union enforces transparency and fairness in AI through GDPR and the AI Act, setting strict compliance standards for AI developers.\",\n",
    "        \n",
    "        \"The United States relies on industry standards and sector-specific regulations rather than a centralized AI fairness law. Federal agencies oversee AI ethics enforcement.\",\n",
    "        \n",
    "        \"China's government-led AI ethics guidelines focus on fairness, transparency, and security. These policies shape the development of ethical AI frameworks.\",\n",
    "        \n",
    "        \"The private sector, including companies like Google, IBM, and Microsoft, invests in AI fairness research, developing frameworks for responsible AI deployment.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "file_paths = []\n",
    "for file_name, parags in documents.items():\n",
    "    doc = Document()\n",
    "    for parag in parags:\n",
    "        doc.add_paragraph(parag)  \n",
    "    file_path = os.path.join(save_directory, file_name)\n",
    "    doc.save(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ftyx_HIB2Jf3"
   },
   "outputs": [],
   "source": [
    "# Extract Chunks using document paragraphs\n",
    "# Chunk size is controlled by parameter\n",
    "\n",
    "def extract_fixed_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Extract fixed-size chunks from a Word document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str or bytes): Path to Word document or binary content\n",
    "        chunk_size (int): Target size of each chunk in characters\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks of approximately chunk_size characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both file path and binary content\n",
    "        if isinstance(file_path, bytes):\n",
    "            doc = Document(BytesIO(file_path))\n",
    "        else:\n",
    "            doc = Document(file_path)\n",
    "\n",
    "        # Extract and clean all text\n",
    "        full_text = \"\"\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:  # Skip empty paragraphs\n",
    "                # Clean the text\n",
    "                text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "                full_text += text + \" \"  # Add space between paragraphs\n",
    "\n",
    "        # Split text into sentences\n",
    "        sentences = re.split('(?<=[.!?]) +', full_text)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If adding this sentence would exceed chunk_size\n",
    "            if len(current_chunk) + len(sentence) > chunk_size:\n",
    "                # If current chunk is not empty, add it to chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "\n",
    "                # Handle sentences longer than chunk_size\n",
    "                if len(sentence) > chunk_size:\n",
    "                    # Split long sentence into fixed-size chunks\n",
    "                    words = sentence.split()\n",
    "                    temp_chunk = \"\"\n",
    "\n",
    "                    for word in words:\n",
    "                        if len(temp_chunk) + len(word) + 1 <= chunk_size:\n",
    "                            temp_chunk += (\" \" + word if temp_chunk else word)\n",
    "                        else:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                            temp_chunk = word\n",
    "\n",
    "                    if temp_chunk:\n",
    "                        current_chunk = temp_chunk\n",
    "                else:\n",
    "                    current_chunk = sentence\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_chunk += (\" \" + sentence if current_chunk else sentence)\n",
    "\n",
    "        # Add the last chunk if not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing document: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fZdcpvLSHCy",
    "outputId": "8c4e1c8a-fc1f-44ff-bfca-d37602eebcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 Word documents\n",
      "\n",
      "Processing: 1.BiasInMachineLearning.docx\n",
      "Created 5 chunks\n",
      "\n",
      "Chunk 1 (length: 215):\n",
      "Bias in machine learning (ML) has evolved alongside AI itself, reflecting the challenges of ensuring fairness in automated decision-making. Early AI systems inherited human biases due to limited representative data.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 268):\n",
      "This document traces the historical development of bias in AI systems, from early statistical models to deep learning and modern AI ethics concerns. Early statistical models (1950s-1980s) relied on manually curated datasets, often reflecting biases in data collection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 214):\n",
      "Linear regression and early statistical learning approaches exhibited bias due to unrepresentative data. Machine learning growth (1990s-2010s) introduced more complex models, increasing reliance on historical data.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 4 (length: 268):\n",
      "Neural networks and decision trees amplified existing biases, leading to issues in credit scoring and facial recognition. The deep learning era (2010s-present) introduced large-scale AI models like GPT and DALL·E, demonstrating biases in language and image generation.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 5 (length: 77):\n",
      "Ethical concerns prompted fairness-aware algorithms and regulatory responses.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 5.Perspectives_EthicalAI.docx\n",
      "Created 4 chunks\n",
      "\n",
      "Chunk 1 (length: 280):\n",
      "AI fairness and ethics are global concerns. This document examines different international approaches to bias mitigation in AI. The United Nations advocates for human rights-based AI governance, emphasizing the importance of fairness and transparency in automated decision-making.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 265):\n",
      "The European Union enforces transparency and fairness in AI through GDPR and the AI Act, setting strict compliance standards for AI developers. The United States relies on industry standards and sector-specific regulations rather than a centralized AI fairness law.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 201):\n",
      "Federal agencies oversee AI ethics enforcement. China's government-led AI ethics guidelines focus on fairness, transparency, and security. These policies shape the development of ethical AI frameworks.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 4 (length: 158):\n",
      "The private sector, including companies like Google, IBM, and Microsoft, invests in AI fairness research, developing frameworks for responsible AI deployment.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 3.RegulatoryApproaches_AI_Bias.docx\n",
      "Created 3 chunks\n",
      "\n",
      "Chunk 1 (length: 240):\n",
      "Governments worldwide are taking different approaches to regulate and mitigate bias in AI. This document explores key regulatory efforts. The European Union has introduced the AI Act (2021), which classifies AI systems based on risk levels.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 288):\n",
      "High-risk systems must meet strict fairness and transparency requirements. The United States lacks a comprehensive AI bias law but relies on the Federal Trade Commission (FTC) to enforce fairness under consumer protection laws. States like Illinois have passed biometric data regulations.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 291):\n",
      "China has implemented AI governance guidelines emphasizing fairness and transparency. Companies deploying AI must ensure non-discrimination in algorithms. Global organizations like UNESCO and OECD advocate for ethical AI principles focusing on non-discrimination, transparency, and fairness.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 2.BiasInMachineLearning_CaseStudies.docx\n",
      "Created 5 chunks\n",
      "\n",
      "Chunk 1 (length: 169):\n",
      "AI bias has resulted in real-world consequences across multiple sectors. This document analyzes major case studies demonstrating the effects of bias in machine learning.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 240):\n",
      "Hiring and employment: Amazon's hiring algorithm (2018) showed gender bias, downgrading female candidates due to past hiring patterns. This case demonstrated how ML models can reinforce historical inequalities if trained on biased datasets.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 252):\n",
      "Law enforcement and criminal justice: The COMPAS algorithm used for criminal sentencing showed racial bias, falsely labeling Black defendants as high risk at twice the rate of white defendants. This sparked concerns over AI-driven legal discrimination.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 4 (length: 253):\n",
      "Healthcare disparities: A healthcare risk-prediction algorithm exhibited racial bias, allocating fewer resources to Black patients with similar health conditions as white patients. This highlights the risk of biased AI in critical decision-making areas.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 5 (length: 222):\n",
      "Financial sector: AI-driven credit scoring systems demonstrated bias against minority communities, leading to disproportionate loan denials. Such biases underscore the need for transparency in AI-based financial decisions.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 4.FairnessTechniquesInML.docx\n",
      "Created 4 chunks\n",
      "\n",
      "Chunk 1 (length: 286):\n",
      "Researchers have developed multiple techniques to reduce AI bias. This document outlines fairness-aware machine learning approaches. Pre-processing methods focus on data balancing techniques such as re-weighting samples and synthetic data generation to improve fairness before training.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 194):\n",
      "In-processing techniques include adversarial debiasing, where models are trained to minimize sensitive attribute influence, and regularization techniques that encourage fair decision boundaries.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 217):\n",
      "Post-processing techniques involve model auditing and explainability tools such as SHAP and LIME, which help interpret biased outcomes. Adjusting model predictions to meet fairness constraints is another key approach.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 4 (length: 142):\n",
      "Fairness-aware algorithms continue to evolve, integrating ethical considerations into AI development to mitigate bias and improve inclusivity.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunks saved to: /home/javad/Downloads/INFO290/content/chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "# Main - Note that chunk size to use is set here in main and overrides default\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Directory containing Word documents\n",
    "        directory = \"/home/javad/Downloads/INFO290/content\"\n",
    "\n",
    "        # Get all .docx files in the directory\n",
    "        docx_files = list(Path(directory).glob(\"*.docx\"))\n",
    "\n",
    "        if not docx_files:\n",
    "            print(f\"No Word documents found in {directory}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(docx_files)} Word documents\")\n",
    "\n",
    "        # Process each document\n",
    "        for doc_path in docx_files:\n",
    "            try:\n",
    "                print(f\"\\nProcessing: {doc_path.name}\")\n",
    "\n",
    "                # Extract chunks of approximately 500 characters\n",
    "                # MODIFY this as you see fit\n",
    "                chunks = extract_fixed_chunks(str(doc_path), chunk_size=300)\n",
    "\n",
    "                print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "                # Print first few chunks with their lengths\n",
    "                for i, chunk in enumerate(chunks[:7], 1):\n",
    "                    print(f\"\\nChunk {i} (length: {len(chunk)}):\")\n",
    "                    print(chunk)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {doc_path.name}: {str(e)}\")\n",
    "                continue\n",
    "        pickle_path = \"/home/javad/Downloads/INFO290/content/chunks.pkl\"\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "\n",
    "        print(f\"\\nChunks saved to: {pickle_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory: {str(e)}\")\n",
    "\n",
    "# Call main and start the chunking\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHBRrpl15FzG"
   },
   "source": [
    "/content/sample_data/mydata"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
