{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNoWkvZacYA4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPFLN63wcZ-y"
   },
   "source": [
    "# Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aD22q2yFdq9B",
    "outputId": "90c68b0a-f775-457a-e797-4286b4401dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /new/benpyenv/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: docx in /new/benpyenv/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: lxml in /new/benpyenv/lib/python3.10/site-packages (from docx) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /new/benpyenv/lib/python3.10/site-packages (from docx) (10.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx\n",
    "!pip install docx\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# from google.colab import files\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /new/benpyenv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /new/benpyenv/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /new/benpyenv/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /new/benpyenv/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /new/benpyenv/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/javad/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from docx import Document\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import docx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUc06yIHezwc",
    "outputId": "e99b70e5-e859-4499-de45-7ee59e8b0d0b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    def __init__(self,\n",
    "                 min_chunk_size: int = 100,\n",
    "                 max_chunk_size: int = 500,\n",
    "                 overlap: int = 50,\n",
    "                 min_sentence_similarity: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the document chunker with configurable parameters.\n",
    "\n",
    "        Args:\n",
    "            min_chunk_size: Minimum characters per chunk\n",
    "            max_chunk_size: Maximum characters per chunk\n",
    "            overlap: Number of characters to overlap between chunks\n",
    "            min_sentence_similarity: Minimum cosine similarity threshold for sentences\n",
    "        \"\"\"\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.min_sentence_similarity = min_sentence_similarity\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def extract_text_from_docx(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text content from a Word document.\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                full_text.append(paragraph.text.strip())\n",
    "        return \"\\n\\n\".join(full_text)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:-]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def calculate_sentence_similarities(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate pairwise cosine similarities between sentences.\n",
    "\n",
    "        Args:\n",
    "            sentences: List of sentences to compare\n",
    "\n",
    "        Returns:\n",
    "            Similarity matrix for sentences\n",
    "        \"\"\"\n",
    "        if not sentences:\n",
    "            return np.array([])\n",
    "\n",
    "        # Create TF-IDF vectors for sentences\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(sentences)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    def find_similar_sentences(self,\n",
    "                             sentences: List[str],\n",
    "                             similarity_matrix: np.ndarray) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Group sentences based on similarity threshold.\n",
    "\n",
    "        Args:\n",
    "            sentences: List of sentences\n",
    "            similarity_matrix: Pairwise similarity matrix\n",
    "\n",
    "        Returns:\n",
    "            List of groups of similar sentence indices\n",
    "        \"\"\"\n",
    "        sentence_groups = []\n",
    "        used_indices = set()\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "\n",
    "            # Start new group with current sentence\n",
    "            current_group = [i]\n",
    "            used_indices.add(i)\n",
    "\n",
    "            # Find similar sentences\n",
    "            for j in range(i + 1, len(sentences)):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "\n",
    "                # Check similarity with all sentences in current group\n",
    "                similarities = [similarity_matrix[j][k] for k in current_group]\n",
    "                if min(similarities) >= self.min_sentence_similarity:\n",
    "                    current_group.append(j)\n",
    "                    used_indices.add(j)\n",
    "\n",
    "            sentence_groups.append(current_group)\n",
    "\n",
    "        return sentence_groups\n",
    "\n",
    "    def create_semantic_chunks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Split text into semantically coherent chunks based on sentence similarity.\n",
    "\n",
    "        Args:\n",
    "            text: Preprocessed text to chunk\n",
    "\n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        # Calculate sentence similarities\n",
    "        similarity_matrix = self.calculate_sentence_similarities(sentences)\n",
    "\n",
    "        # Group similar sentences\n",
    "        sentence_groups = self.find_similar_sentences(sentences, similarity_matrix)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for group in sentence_groups:\n",
    "            group_sentences = [sentences[i] for i in group]\n",
    "            group_text = ' '.join(group_sentences)\n",
    "            group_length = len(group_text)\n",
    "\n",
    "            # Check if adding this group would exceed max chunk size\n",
    "            if current_length + group_length > self.max_chunk_size and current_chunk:\n",
    "                # Create chunk from accumulated sentences\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'length': len(chunk_text),\n",
    "                    'sentences': len(current_chunk),\n",
    "                    'avg_group_similarity': np.mean([\n",
    "                        similarity_matrix[i][j]\n",
    "                        for i in range(len(current_chunk))\n",
    "                        for j in range(i + 1, len(current_chunk))\n",
    "                    ]) if len(current_chunk) > 1 else 1.0\n",
    "                })\n",
    "\n",
    "                # Start new chunk\n",
    "                current_chunk = group_sentences\n",
    "                current_length = group_length\n",
    "            else:\n",
    "                current_chunk.extend(group_sentences)\n",
    "                current_length += group_length\n",
    "\n",
    "        # Add final chunk if it meets minimum size\n",
    "        if current_length >= self.min_chunk_size:\n",
    "            final_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'text': final_text,\n",
    "                'length': len(final_text),\n",
    "                'sentences': len(current_chunk),\n",
    "                'avg_group_similarity': np.mean([\n",
    "                    similarity_matrix[i][j]\n",
    "                    for i in range(len(current_chunk))\n",
    "                    for j in range(i + 1, len(current_chunk))\n",
    "                ]) if len(current_chunk) > 1 else 1.0\n",
    "            })\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Process a Word document end-to-end.\"\"\"\n",
    "        raw_text = self.extract_text_from_docx(file_path)\n",
    "        processed_text = self.preprocess_text(raw_text)\n",
    "        return self.create_semantic_chunks(processed_text)\n",
    "\n",
    "    def write_chunks_to_file(self, chunks: List[Dict], output_path: str = \"/content/mychunk.txt\"):\n",
    "        \"\"\"Write chunks to a text file with metadata.\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                f.write(f\"{'='*80}\\n\")\n",
    "                f.write(f\"CHUNK {i}\\n\")\n",
    "                f.write(f\"Length: {chunk['length']} characters\\n\")\n",
    "                f.write(f\"Sentences: {chunk['sentences']}\\n\")\n",
    "                f.write(f\"Average group similarity: {chunk['avg_group_similarity']:.3f}\\n\")\n",
    "                f.write(f\"{'-'*40}\\n\")\n",
    "                f.write(f\"{chunk['text']}\\n\\n\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at '/content/TheEvolutionofPrivacy.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m chunker \u001b[38;5;241m=\u001b[39m DocumentChunker(\n\u001b[1;32m      3\u001b[0m     min_chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      4\u001b[0m     max_chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      5\u001b[0m     overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      6\u001b[0m     min_sentence_similarity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m  \u001b[38;5;66;03m# Adjust this threshold as needed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Process a document\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_document\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/TheEvolutionofPrivacy.docx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m chunker\u001b[38;5;241m.\u001b[39mwrite_chunks_to_file(chunks)\n",
      "Cell \u001b[0;32mIn[5], line 166\u001b[0m, in \u001b[0;36mDocumentChunker.process_document\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_document\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict]:\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process a Word document end-to-end.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     raw_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_from_docx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_text(raw_text)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_semantic_chunks(processed_text)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mDocumentChunker.extract_text_from_docx\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_docx\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract text content from a Word document.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdocx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs:\n",
      "File \u001b[0;32m/new/benpyenv/lib/python3.10/site-packages/docx/api.py:27\u001b[0m, in \u001b[0;36mDocument\u001b[0;34m(docx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[0;32m---> 27\u001b[0m document_part \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentPart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[1;32m     29\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/new/benpyenv/lib/python3.10/site-packages/docx/opc/package.py:127\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpcPackage:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPackageReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    129\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[0;32m/new/benpyenv/lib/python3.10/site-packages/docx/opc/pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[0;34m(pkg_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[1;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[0;32m/new/benpyenv/lib/python3.10/site-packages/docx/opc/phys_pkg.py:21\u001b[0m, in \u001b[0;36mPhysPkgReader.__new__\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m         reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: Package not found at '/content/TheEvolutionofPrivacy.docx'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chunker = DocumentChunker(\n",
    "        min_chunk_size=200,\n",
    "        max_chunk_size=500,\n",
    "        overlap=100,\n",
    "        min_sentence_similarity=0.3  # Adjust this threshold as needed\n",
    "    )\n",
    "\n",
    "    # Process a document\n",
    "    chunks = chunker.process_document(\"/content/TheEvolutionofPrivacy.docx\")\n",
    "    chunker.write_chunks_to_file(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "H4ZI7uUoceps",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "302f9787-5b1a-469f-e4c1-789c9d317083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentence: Knowledge graph embedding methods for entity alignment: experimental review Paper Review Eleventh Hour Enthusiast · 10 min read · Oct 12, 2024 Introduction Knowledge graphs (KGs) have become essential in various domains, powering applications such as question answering, recommendations, and semantic search.\n",
      "\n",
      "Similarity ranking:\n",
      "\n",
      "Similarity: 0.4524\n",
      "Sentence: Knowledge graph embedding methods for entity alignment: experimental review.\n",
      "\n",
      "Similarity: 0.1838\n",
      "Sentence: Methods Used for Entity Alignment The paper evaluates several methods for entity alignment using knowledge graph embeddings.\n",
      "\n",
      "Similarity: 0.1445\n",
      "Sentence: MTransE is one of the foundational supervised methods for entity alignment across knowledge graphs.\n",
      "\n",
      "Similarity: 0.1197\n",
      "Sentence: The study’s experimental evaluation highlights the strengths and trade-offs of various embedding-based entity alignment methods, revealing statistically significant rankings and correlations with dataset characteristics.\n",
      "\n",
      "Similarity: 0.1104\n",
      "Sentence: MTransE extends this concept to align entities across different knowledge graphs, typically those in different languages or domains.\n",
      "\n",
      "Similarity: 0.1062\n",
      "Sentence: After training, MTransE can align new entities by embedding them in their original knowledge graph space, applying the learned alignment transformation, and finding the nearest entity in the target knowledge graph space.\n",
      "\n",
      "Similarity: 0.0967\n",
      "Sentence: By combining rigorous statistical analysis with a diverse testbed, the paper offers a comprehensive evaluation of embedding-based entity alignment methods.\n",
      "\n",
      "Similarity: 0.0841\n",
      "Sentence: During training, MTransE jointly optimizes the knowledge graph embeddings and the alignment.\n",
      "\n",
      "Similarity: 0.0718\n",
      "Sentence: This method combines the strengths of GCNs with a novel relational reflection technique, resulting in a more refined and effective approach to entity alignment across knowledge graphs.\n",
      "\n",
      "Similarity: 0.0707\n",
      "Sentence: The core principle of MTransE is that similar entities in different knowledge graphs should have similar representations in a unified embedding space.\n",
      "\n",
      "Similarity: 0.0697\n",
      "Sentence: These tests provide a statistically sound methodology for analyzing the performance of different methods, revealing significant correlations with various meta-features extracted from the KGs.\n",
      "\n",
      "Similarity: 0.0694\n",
      "Sentence: Relation-based methods focus on leveraging the structural information within KGs.\n",
      "\n",
      "Similarity: 0.0691\n",
      "Sentence: MTransE+RotatE represents a significant enhancement to the original MTransE model, addressing some of its limitations while preserving its core strengths in entity alignment across knowledge graphs.\n",
      "\n",
      "Similarity: 0.0686\n",
      "Sentence: Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs. arXiv e-print, arXiv:1908.08210. https://doi.org/10.48550/arXiv.1908.08210\n",
      "\n",
      "Similarity: 0.0681\n",
      "Sentence: In many knowledge graphs, symmetric relations, such as “is similar to”, are common and important.\n",
      "\n",
      "Similarity: 0.0674\n",
      "Sentence: This approach involves learning a transformation function to map entity embeddings from one knowledge graph to another.\n",
      "\n",
      "Similarity: 0.0674\n",
      "Sentence: However, integrating knowledge from different KGs presents significant challenges, primarily due to the need to identify and align subgraphs that refer to the same real-world entities — a process known as entity alignment.\n",
      "\n",
      "Similarity: 0.0668\n",
      "Sentence: Data Min Knowl Disc 37, 2070–2137 (2023). https://doi.org/10.1007/s10618-023-00941-9 Mao, X., Wang, W., Xu, H., Wu, Y., & Lan, M. (2020).\n",
      "\n",
      "Similarity: 0.0666\n",
      "Sentence: This joint learning process enables the model to create embeddings that are both meaningful within each knowledge graph and comparable across different graphs.\n",
      "\n",
      "Similarity: 0.0657\n",
      "Sentence: This mechanism assigns different weights to various parts of the graph, helping the model focus on the most relevant information for each entity.\n",
      "\n",
      "Similarity: 0.0652\n",
      "Sentence: Motivation and Key Research Questions The main challenge in integrating knowledge from different KGs is identifying and aligning subgraphs that correspond to the same real-world entities, a task known as entity alignment.\n",
      "\n",
      "Similarity: 0.0636\n",
      "Sentence: Relational reflection entity alignment.\n",
      "\n",
      "Similarity: 0.0629\n",
      "Sentence: These findings underscore the importance of considering both structural and attribute information in embedding methods to achieve robust entity alignment.\n",
      "\n",
      "Similarity: 0.0624\n",
      "Sentence: This approach aims to directly overlap the embeddings of aligned entities from different knowledge graphs.\n",
      "\n",
      "Similarity: 0.0620\n",
      "Sentence: Both versions of RREA use a sharing alignment technique, which aims to directly overlap the embeddings of aligned entities from different knowledge graphs.\n",
      "\n",
      "Similarity: 0.0606\n",
      "Sentence: This initial step preserves the structural information within each knowledge graph.\n",
      "\n",
      "Similarity: 0.0603\n",
      "Sentence: These methods span supervised, semi-supervised, and unsupervised approaches, and utilize both relational and attribute information from the knowledge graphs.\n",
      "\n",
      "Similarity: 0.0585\n",
      "Sentence: The paper by Fanourakis et al. addresses this gap by conducting a meta-level analysis of popular embedding methods, providing valuable insights into their effectiveness and efficiency.\n",
      "\n",
      "Similarity: 0.0564\n",
      "Sentence: This method is designed to capture and utilize multi-hop structural information within knowledge graphs, offering a more comprehensive understanding of entity contexts.\n",
      "\n",
      "Similarity: 0.0561\n",
      "Sentence: RREA (Relational Reflection Entity Alignment).\n",
      "\n",
      "Similarity: 0.0542\n",
      "Sentence: Methodology The paper extends the benchmark of datasets with pairs of KGs typically used in empirical studies, including five additional datasets with unique characteristics.\n",
      "\n",
      "Similarity: 0.0518\n",
      "Sentence: This improvement allows MTransE+RotatE to capture the complex relational structures within knowledge graphs with greater precision.\n",
      "\n",
      "Similarity: 0.0515\n",
      "Sentence: The meta-level analysis examines the correlations between the performance of the methods and various dataset characteristics, revealing how different methods perform under varying conditions and providing insights into their strengths and weaknesses.\n",
      "\n",
      "Similarity: 0.0509\n",
      "Sentence: The translation vector method learns a vector to move from one knowledge graph’s space to another.\n",
      "\n",
      "Similarity: 0.0503\n",
      "Sentence: To achieve this, MTransE first uses TransE to embed each knowledge graph separately, aiming to satisfy the condition h + r ≈ t for each triple (head, relation, tail) in the graph.\n",
      "\n",
      "Similarity: 0.0490\n",
      "Sentence: The linear transformation technique learns a matrix to transform entities between knowledge graph spaces.\n",
      "\n",
      "Similarity: 0.0483\n",
      "Sentence: To bridge the gap between different knowledge graph embeddings, MTransE introduces three alignment strategies: distance-based axis calibration, translation vectors, and linear transformation.\n",
      "\n",
      "Similarity: 0.0481\n",
      "Sentence: This architecture involves two interconnected graphs: a primal graph, which represents entities and their relationships, and a dual graph, where the nodes correspond to relation types.\n",
      "\n",
      "Similarity: 0.0461\n",
      "Sentence: Conclusion The comprehensive analysis presented in this paper offers significant insights.\n",
      "\n",
      "Similarity: 0.0448\n",
      "Sentence: While MTransE’s ability to preserve the structure of each knowledge graph while aligning entities is a significant strength, it does have limitations.\n",
      "\n",
      "Similarity: 0.0445\n",
      "Sentence: This detailed methodology helps practitioners understand which methods are best suited for different types of KGs and alignment tasks, ultimately guiding the development of more effective and efficient solutions.\n",
      "\n",
      "Similarity: 0.0440\n",
      "Sentence: This process helps to push dissimilar entities further apart in the embedding space, leading to more discriminative entity representations and improved alignment accuracy.\n",
      "\n",
      "Similarity: 0.0434\n",
      "Sentence: It uses a margin-based loss function that combines the TransE loss for each knowledge graph with the alignment loss.\n",
      "\n",
      "Similarity: 0.0428\n",
      "Sentence: The method’s reliance on TransE embeddings and its lack of negative sampling can restrict its performance, especially when dealing with very large or sparse knowledge graphs.\n",
      "\n",
      "Similarity: 0.0427\n",
      "Sentence: By encouraging aligned entities to have nearly identical representations in the shared embedding space, the model facilitates more accurate cross-graph entity matching.\n",
      "\n",
      "Similarity: 0.0422\n",
      "Sentence: While GCNs excel at capturing structural information, they often struggle to differentiate between various types of relations.\n",
      "\n",
      "Similarity: 0.0417\n",
      "Sentence: This combination allows RREA to capture both the overall structure of the knowledge graph and the fine-grained, relation-specific details.\n",
      "\n",
      "Similarity: 0.0413\n",
      "Sentence: This self-bootstrapping approach allows RREA(semi) to leverage a larger portion of the knowledge graph for alignment, potentially leading to better performance, especially when the initial set of known alignments is small.\n",
      "\n",
      "Similarity: 0.0393\n",
      "Sentence: By leveraging this factual data, attribute-based methods can provide more contextually rich representations of entities, which is crucial for KGs with extensive attribute information.\n",
      "\n",
      "Similarity: 0.0386\n",
      "Sentence: This diverse testbed enables the authors to draw new insights regarding the evaluated entity alignment (EA) methods.\n",
      "\n",
      "Similarity: 0.0385\n",
      "Sentence: Embedding methods, which learn vector-space representations of entities, have emerged as powerful tools for this task.\n",
      "\n",
      "Similarity: 0.0372\n",
      "Sentence: Relation-Based Methods MTransE.\n",
      "\n",
      "Similarity: 0.0368\n",
      "Sentence: This improved version replaces the TransE embedding component with the more sophisticated RotatE model, introducing a novel approach to representing relations in the embedding space.\n",
      "\n",
      "Similarity: 0.0336\n",
      "Sentence: For entity alignment, RDGCN adopts a mapping-based technique, similar to MTransE.\n",
      "\n",
      "Similarity: 0.0323\n",
      "Sentence: These methods utilize the connections between entities (i.e., relationships or edges) to learn embeddings that reflect the graph’s topology.\n",
      "\n",
      "Similarity: 0.0317\n",
      "Sentence: RDGCN (Wu et al., 2019) introduces a novel approach to entity alignment, departing from translation-based models like MTransE and RotatE by leveraging the capabilities of graph neural networks.\n",
      "\n",
      "Similarity: 0.0311\n",
      "Sentence: This dual-graph structure enables RDGCN to model both entity-to-entity and relation-to-relation interactions simultaneously, capturing complex relational patterns that might be overlooked by simpler models.\n",
      "\n",
      "Similarity: 0.0300\n",
      "Sentence: Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3340531.3412001 Wu, Y., Liu, X., Feng, Y., Wang, Z., Yan, R., & Zhao, D. (2019).\n",
      "\n",
      "Similarity: 0.0300\n",
      "Sentence: RREA (Mao et al., 2020) represents a significant advancement in the field of entity alignment, building upon the success of graph neural network approaches while introducing innovative mechanisms to enhance performance.\n",
      "\n",
      "Similarity: 0.0293\n",
      "Sentence: While embedding methods have shown potential in tackling this challenge, comprehensive evaluations comparing their strengths and limitations across diverse real-world datasets remain limited.\n",
      "\n",
      "Similarity: 0.0287\n",
      "Sentence: In this framework, each relation is modeled as a rotation from the head entity to the tail entity.\n",
      "\n",
      "Similarity: 0.0278\n",
      "Sentence: It builds upon TransE, which represents entities and relations as vectors in a shared embedding space, where relations are interpreted as translations between entity vectors.\n",
      "\n",
      "Similarity: 0.0266\n",
      "Sentence: They are particularly effective in capturing the intricate network of relations in dense KGs, making them suitable for tasks heavily reliant on relational patterns.\n",
      "\n",
      "Similarity: 0.0261\n",
      "Sentence: By doing so, RREA can create more precise and context-aware entity representations, leading to improved alignment accuracy.\n",
      "\n",
      "Similarity: 0.0258\n",
      "Sentence: Unlike earlier methods that focus on immediate relationships, RDGCN aggregates information from a broader neighborhood around each entity.\n",
      "\n",
      "Similarity: 0.0256\n",
      "Sentence: It learns to align entities by minimizing the distance between known counterparts in the embedding space.\n",
      "\n",
      "Similarity: 0.0246\n",
      "Sentence: At its core, RDGCN utilizes Graph Convolutional Networks (GCNs), a class of neural networks specialized for operating on graph-structured data.\n",
      "\n",
      "Similarity: 0.0244\n",
      "Sentence: RDGCN (Relational Dual-Graph Convolutional Network).\n",
      "\n",
      "Similarity: 0.0241\n",
      "Sentence: The distance-based approach minimizes the distance between aligned entities in the embedding space.\n",
      "\n",
      "Similarity: 0.0235\n",
      "Sentence: RDGCN further incorporates an attention mechanism to facilitate information exchange between the primal and dual graphs.\n",
      "\n",
      "Similarity: 0.0233\n",
      "Sentence: Despite these constraints, MTransE remains an important benchmark in the field of entity alignment, paving the way for more advanced techniques.\n",
      "\n",
      "Similarity: 0.0231\n",
      "Sentence: In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM ’20) (pp. 1095–1104).\n",
      "\n",
      "Similarity: 0.0229\n",
      "Sentence: These strategies allow MTransE to create a unified framework for cross-lingual or cross-domain entity alignment.\n",
      "\n",
      "Similarity: 0.0228\n",
      "Sentence: A key feature of RDGCN is its dual-graph architecture.\n",
      "\n",
      "Similarity: 0.0227\n",
      "Sentence: The model stacks multiple GNN layers to aggregate information from multi-hop neighborhoods, providing a comprehensive view of each entity’s context within the graph.\n",
      "\n",
      "Similarity: 0.0224\n",
      "Sentence: Which dataset characteristics affect the performance of different methods?\n",
      "\n",
      "Similarity: 0.0216\n",
      "Sentence: These graphs store machine-readable descriptions of real-world entities, capturing both relational and factual information.\n",
      "\n",
      "Similarity: 0.0201\n",
      "Sentence: For the alignment process, MTransE+RotatE employs a sharing technique.\n",
      "\n",
      "Similarity: 0.0184\n",
      "Sentence: This negative sampling strategy aids the model in learning more discriminative features by pushing dissimilar entities farther apart in the embedding space.\n",
      "\n",
      "Similarity: 0.0181\n",
      "Sentence: Attribute-based methods, on the other hand, utilize the literal information associated with entities.\n",
      "\n",
      "Similarity: 0.0177\n",
      "Sentence: How do these methods benefit from jointly considering structural relations and attribute values?\n",
      "\n",
      "Similarity: 0.0170\n",
      "Sentence: This approach, combined with the relational reflection and attention mechanisms, allows RREA to create a unified embedding space where aligned entities are positioned close to each other while maintaining their relational contexts.\n",
      "\n",
      "Similarity: 0.0160\n",
      "Sentence: The model’s architecture integrates both GCNs and Graph Attention Networks (GATs), leveraging their complementary strengths.\n",
      "\n",
      "Similarity: 0.0149\n",
      "Sentence: At the core of RREA is the relational reflection mechanism, which addresses a key limitation of previous GCN-based methods.\n",
      "\n",
      "Similarity: 0.0149\n",
      "Sentence: These methods incorporate attributes (such as names, descriptions, and other textual or numerical data) to enhance the embeddings.\n",
      "\n",
      "Similarity: 0.0143\n",
      "Sentence: The authors explore several key questions: What factors influence the performance of relation-based and attribute-based methods?\n",
      "\n",
      "Similarity: 0.0142\n",
      "Sentence: Statistical methods such as the Friedman test and the Nemenyi test are used to ensure a comprehensive evaluation.\n",
      "\n",
      "Similarity: 0.0134\n",
      "Sentence: RREA(semi), on the other hand, is a semi-supervised variant that can operate with limited initial alignment information.\n",
      "\n",
      "Similarity: 0.0132\n",
      "Sentence: Despite numerous proposed methods, there has been a lack of comprehensive evaluations comparing their strengths and weaknesses across diverse real-world datasets.\n",
      "\n",
      "Similarity: 0.0130\n",
      "Sentence: RREA(basic) is a supervised version that requires a set of known entity alignments for training.\n",
      "\n",
      "Similarity: 0.0125\n",
      "Sentence: However, RDGCN’s use of GCNs provides richer entity representations, resulting in more accurate alignments.\n",
      "\n",
      "Similarity: 0.0114\n",
      "Sentence: Consequently, RDGCN can filter out noise and prioritize the most informative relationships and attributes when constructing entity embeddings.\n",
      "\n",
      "Similarity: 0.0110\n",
      "Sentence: This multi-hop capability allows the model to capture more complex and distant relationships, enriching the context for entity representation.\n",
      "\n",
      "Similarity: 0.0110\n",
      "Sentence: The Nemenyi test, a post-hoc analysis following the Friedman test, performs pairwise comparisons to determine which methods differ significantly.\n",
      "\n",
      "Similarity: 0.0102\n",
      "Sentence: This attention-based approach allows the model to dynamically focus on the most relevant information for each entity, effectively filtering out noise from less important or irrelevant connections.\n",
      "\n",
      "Similarity: 0.0099\n",
      "Sentence: This is achieved through an orthogonal transformation matrix for each relation, which “reflects” entity embeddings across different relational hyperplanes while preserving their norms and relative distances.\n",
      "\n",
      "Similarity: 0.0097\n",
      "Sentence: This technique involves generating “false” triples by randomly replacing either the head or tail entity in known true triples.\n",
      "\n",
      "Similarity: 0.0090\n",
      "Sentence: The Friedman test, a non-parametric statistical test, detects differences in treatments across multiple test attempts, making it suitable for comparing multiple methods on different datasets.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: This post explores the findings and implications of their research.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: Is the runtime overhead of each method justified by its effectiveness?\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: The evaluation protocol involves preprocessing the datasets for consistency and applying each method to the same set of datasets.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: Effectiveness is measured using metrics like precision, recall, and F1 score, while efficiency is evaluated based on runtime and computational overhead.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: MTransE+RotatE.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: At the core of this enhancement is the RotatE model, which represents relations as rotations in a complex vector space.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: Mathematically, for a triple (head, relation, tail), RotatE aims to satisfy the condition t = h ◦ r, where ◦ denotes the Hadamard (element-wise) product.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: This representation enables RotatE to capture a broader range of relation patterns compared to TransE, including symmetry, inversion, and composition.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: The ability to model symmetric relations is a particular strength of RotatE.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: While TransE struggled with these types of relations, RotatE can represent them naturally as rotations of 0 or π in the complex plane.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: Another important advancement in MTransE+RotatE is the integration of negative sampling.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: During training, the model learns to assign higher scores to true triples and lower scores to these generated negative samples.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: To address the increased complexity of its embeddings, RDGCN relies on a higher number of negative samples during training.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: While this approach increases computational requirements, it enhances the model’s ability to make fine-grained distinctions between entities.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: RREA’s relational reflection allows the model to learn relation-specific transformations, effectively encoding the unique characteristics of each relation type.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: RREA incorporates an attention mechanism that plays an important role in weighing the importance of different relations and neighboring entities.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: RREA comes in two variants, catering to different scenarios and data availability.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: This version iteratively proposes new alignments based on the current model state, gradually expanding the training set.\n",
      "\n",
      "Similarity: 0.0000\n",
      "Sentence: References: Fanourakis, N., Efthymiou, V., Kotzinos, D. et al.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
