{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NqMv3iTNUGM",
    "outputId": "c89f85bd-2769-4e9e-b710-65f1bb8558bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /new/benpyenv/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /new/benpyenv/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: docx in /new/benpyenv/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: lxml in /new/benpyenv/lib/python3.10/site-packages (from docx) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /new/benpyenv/lib/python3.10/site-packages (from docx) (10.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "!pip install python-docx\n",
    "!pip install docx\n",
    "\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# from google.colab import files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YzICc7GNDQS"
   },
   "source": [
    "# RAG Mini Project\n",
    "## Milestone #1 : Create and store Chunks\n",
    "This notebook shows how to create text chunks from MS Word Documents.  \n",
    "- Chunks all the word documents in a directory\n",
    "- Uses python-docs to extract paragraph text for chunking\n",
    "- Paragraphs are merged depending on parameterizable  max chunk size\n",
    "- Document cleaning recommended for best results\n",
    "-- remove diagrams and unnecessary text\n",
    "-- merge paragraphs that are semantically similar\n",
    "\n",
    "## Deliverables:\n",
    "- Selection of multiple documents for your RAG project\n",
    "- Capture chunks in a pickle file for next step (Embeddings)\n",
    "\n",
    "When you generate the chunks with the size and semantics you want, then store them into a List and use Python's pickle library to save any Python data structure in a pickle file (.pkl) for later use and then recreate the data structure by loading the pickle file.\n",
    "\n",
    "Deliverables:\n",
    "\n",
    "- List of documents for your RAG project\n",
    "- Jupyter Notebook\n",
    "- Short summary of your efforts (issues, successes...)\n",
    "- Pickle file with chunks for next step (Milestone 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How to use pickle (good for Python data)\n",
    "<code>import pickle<br>\n",
    "my_list = ['apple', 'banana', 42, [1, 2, 3]]</code>\n",
    "\n",
    "### Save the list\n",
    "<code>with open('mylist.pkl', 'wb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; pickle.dump(my_list, f)\n",
    "</code>\n",
    "### Load the list\n",
    "<code>with open('mylist.pkl', 'rb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; loaded_list = pickle.load(f)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4jvsOqwMpQa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ftyx_HIB2Jf3"
   },
   "outputs": [],
   "source": [
    "# Extract Chunks using document paragraphs\n",
    "# Chunk size is controlled by parameter\n",
    "\n",
    "def extract_fixed_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Extract fixed-size chunks from a Word document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str or bytes): Path to Word document or binary content\n",
    "        chunk_size (int): Target size of each chunk in characters\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks of approximately chunk_size characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both file path and binary content\n",
    "        if isinstance(file_path, bytes):\n",
    "            doc = Document(BytesIO(file_path))\n",
    "        else:\n",
    "            doc = Document(file_path)\n",
    "\n",
    "        # Extract and clean all text\n",
    "        full_text = \"\"\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:  # Skip empty paragraphs\n",
    "                # Clean the text\n",
    "                text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "                full_text += text + \" \"  # Add space between paragraphs\n",
    "\n",
    "        # Split text into sentences\n",
    "        sentences = re.split('(?<=[.!?]) +', full_text)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If adding this sentence would exceed chunk_size\n",
    "            if len(current_chunk) + len(sentence) > chunk_size:\n",
    "                # If current chunk is not empty, add it to chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "\n",
    "                # Handle sentences longer than chunk_size\n",
    "                if len(sentence) > chunk_size:\n",
    "                    # Split long sentence into fixed-size chunks\n",
    "                    words = sentence.split()\n",
    "                    temp_chunk = \"\"\n",
    "\n",
    "                    for word in words:\n",
    "                        if len(temp_chunk) + len(word) + 1 <= chunk_size:\n",
    "                            temp_chunk += (\" \" + word if temp_chunk else word)\n",
    "                        else:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                            temp_chunk = word\n",
    "\n",
    "                    if temp_chunk:\n",
    "                        current_chunk = temp_chunk\n",
    "                else:\n",
    "                    current_chunk = sentence\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_chunk += (\" \" + sentence if current_chunk else sentence)\n",
    "\n",
    "        # Add the last chunk if not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing document: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fZdcpvLSHCy",
    "outputId": "8c4e1c8a-fc1f-44ff-bfca-d37602eebcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 Word documents\n",
      "\n",
      "Processing: 5.Global Approaches to Data Protection.docx\n",
      "Created 4 chunks\n",
      "\n",
      "Chunk 1 (length: 293):\n",
      "Global Approaches to Data Protection: A Comparative Analysis Data protection approaches vary significantly across different regions of the world, reflecting diverse cultural, political, and economic priorities. This document examines how different regions approach privacy and data protection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 495):\n",
      "European Union Approach: Comprehensive Protection: - GDPR as global standard - Privacy as fundamental right - Strict consent requirements - Significant penalties - Data Protection Authorities Key Features: - Data minimization principles - Purpose limitation - Storage limitations - Individual rights emphasis - Cross-border transfer restrictions United States Approach: Sectoral Regulation: - Industry-specific laws - State-level legislation - FTC enforcement - Market-driven solutions - Limited\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 498):\n",
      "federal oversight Key Features: - Consumer protection focus - Industry self-regulation - State-level innovation - Breach notification requirements - Sectoral compliance frameworks Asia-Pacific Approaches: China: - Personal Information Protection Law (2021) - Cybersecurity Law (2017) - Data localization requirements - State security emphasis - Strict cross-border data rules Japan: - Act on Protection of Personal Information - GDPR adequacy decision - Balanced approach - Cultural privacy norms -\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 2.DevelopmentPrivacyProtectionUSA.docx\n",
      "Created 11 chunks\n",
      "\n",
      "Chunk 1 (length: 396):\n",
      "The Development of Privacy Protection in the United States: A Sectoral Approach The United States has historically taken a markedly different approach to privacy protection compared to Europe, developing a patchwork of sector-specific laws rather than comprehensive federal legislation. This sectoral approach reflects American values of free market economics and limited government intervention.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 222):\n",
      "Early Privacy Developments (1960s-1970s): The Fair Credit Reporting Act of 1970 was one of the first federal laws addressing privacy concerns, focusing specifically on the collection and use of consumer credit information.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 325):\n",
      "This law established important principles including: - Consumer right to access their credit reports - Requirement for accurate reporting - Time limits on negative information - Procedures for disputing incorrect information The Privacy Act of 1974 represented another significant step, though limited to government agencies.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 3.Major Data Breaches and Their Impact on Privacy Regulation.docx\n",
      "Created 8 chunks\n",
      "\n",
      "Chunk 1 (length: 301):\n",
      "Major Data Breaches and Their Impact on Privacy Regulation The history of data protection has been significantly shaped by major data breaches, each contributing to evolving privacy regulations and security practices. This document examines key breaches and their lasting impact on privacy protection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 500):\n",
      "Early Notable Breaches (1984-2000): The TRW Credit Data breach in 1984 exposed 90 million credit records, leading to: - Enhanced security requirements for credit reporting agencies - Creation of consumer protection measures - Implementation of access controls The emergence of e-commerce in the 1990s brought new vulnerabilities: - CD Universe breach (1999) - 300,000 credit card numbers exposed - Egghead.com breach (2000) - 3.7 million customer records compromised These early incidents highlighted\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 499):\n",
      "the need for: - Encryption of stored data - Secure transaction processing - Regular security audits Major Breaches of the 2000s: 2005 ChoicePoint Breach: - 163,000 consumer records compromised - $15 million in fines - Led to first state breach notification law in California - Sparked national discussion on data broker regulation 2007 TJX Companies Breach: - 45.7 million credit card numbers stolen - $256 million in costs - Resulted in: - PCI DSS compliance enhancement - Improved payment security\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 4.The Evolution of European Data Protection.docx\n",
      "Created 10 chunks\n",
      "\n",
      "Chunk 1 (length: 416):\n",
      "The Evolution of European Data Protection: From Privacy Rights to GDPR The European approach to data protection and privacy has its roots in the aftermath of World War II, when privacy was recognized as a fundamental human right in the 1950 European Convention on Human Rights. This early foundation would shape decades of European privacy legislation and establish a distinctly European approach to data protection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 478):\n",
      "The first significant European data protection law emerged in 1970, when the German state of Hesse passed the world's first data protection statute. This pioneering legislation established core principles that would later influence wider European data protection frameworks, including the concepts of data minimization and purpose limitation. The 1981 Council of Europe Convention 108 marked the first legally binding international treaty addressing privacy and data protection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 355):\n",
      "It required signatories to enact legislation concerning the automatic processing of personal data and established key principles such as: - Fair and lawful data collection and processing - Storage limitation - Data accuracy requirements - Appropriate security measures A major milestone came in 1995 with the European Data Protection Directive (95/46/EC).\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: 1.The Evolution of Privacy.docx\n",
      "Created 8 chunks\n",
      "\n",
      "Chunk 1 (length: 324):\n",
      "The Evolution of Privacy-Enhancing Technologies Privacy-enhancing technologies (PETs) have evolved significantly since the early days of digital communication, reflecting growing concerns about data protection and privacy. This document traces the development of key privacy technologies and their impact on data protection.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 497):\n",
      "Early Encryption Technologies (1970s-1980s): Data Encryption Standard (DES): - Developed by IBM in 1974 - Standardized by NIST in 1977 - 56-bit key length - Used extensively in financial transactions - Eventually replaced due to security concerns Public Key Cryptography: - Diffie-Hellman key exchange (1976) - RSA algorithm (1977) - Enabled secure communication without pre-shared keys - Fundamental to modern secure communications 1990s Developments: Pretty Good Privacy (PGP): - Created by Phil\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 491):\n",
      "Zimmermann in 1991 - Provided email encryption and digital signatures - Led to significant privacy debates - Influenced modern end-to-end encryption SSL/TLS Evolution: - SSL 2.0 released by Netscape (1995) - SSL 3.0 (1996) - TLS 1.0 (1999) - Enabled secure online commerce - Became foundation for HTTPS Early 2000s Innovations: Privacy-Preserving Data Mining: - Developed in response to growing data collection - Techniques for anonymizing datasets - Statistical disclosure control methods -\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main - Note that chunk size to use is set here in main and overrides default\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Directory containing Word documents\n",
    "        directory = \"/home/javad/Downloads/INFO290/content\"\n",
    "\n",
    "        # Get all .docx files in the directory\n",
    "        docx_files = list(Path(directory).glob(\"*.docx\"))\n",
    "\n",
    "        if not docx_files:\n",
    "            print(f\"No Word documents found in {directory}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(docx_files)} Word documents\")\n",
    "\n",
    "        # Process each document\n",
    "        for doc_path in docx_files:\n",
    "            try:\n",
    "                print(f\"\\nProcessing: {doc_path.name}\")\n",
    "\n",
    "                # Extract chunks of approximately 500 characters\n",
    "                # MODIFY this as you see fit\n",
    "                chunks = extract_fixed_chunks(str(doc_path), chunk_size=500)\n",
    "\n",
    "                print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "                # Print first few chunks with their lengths\n",
    "                for i, chunk in enumerate(chunks[:3], 1):\n",
    "                    print(f\"\\nChunk {i} (length: {len(chunk)}):\")\n",
    "                    print(chunk)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {doc_path.name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory: {str(e)}\")\n",
    "\n",
    "# Call main and start the chunking\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHBRrpl15FzG"
   },
   "source": [
    "/content/sample_data/mydata"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
