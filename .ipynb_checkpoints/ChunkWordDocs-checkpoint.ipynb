{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NqMv3iTNUGM",
    "outputId": "c89f85bd-2769-4e9e-b710-65f1bb8558bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: docx in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "!pip install python-docx\n",
    "!pip install docx\n",
    "\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YzICc7GNDQS"
   },
   "source": [
    "# RAG Mini Project\n",
    "## Milestone #1 : Create and store Chunks\n",
    "This notebook shows how to create text chunks from MS Word Documents.  \n",
    "- Chunks all the word documents in a directory\n",
    "- Uses python-docs to extract paragraph text for chunking\n",
    "- Paragraphs are merged depending on parameterizable  max chunk size\n",
    "- Document cleaning recommended for best results\n",
    "-- remove diagrams and unnecessary text\n",
    "-- merge paragraphs that are semantically similar\n",
    "\n",
    "## Deliverables:\n",
    "- Selection of multiple documents for your RAG project\n",
    "- Capture chunks in a pickle file for next step (Embeddings)\n",
    "\n",
    "When you generate the chunks with the size and semantics you want, then store them into a List and use Python's pickle library to save any Python data structure in a pickle file (.pkl) for later use and then recreate the data structure by loading the pickle file.\n",
    "\n",
    "### How to use pickle (good for Python data)\n",
    "<code>import pickle<br>\n",
    "my_list = ['apple', 'banana', 42, [1, 2, 3]]</code>\n",
    "\n",
    "### Save the list\n",
    "<code>with open('mylist.pkl', 'wb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; pickle.dump(my_list, f)\n",
    "</code>\n",
    "### Load the list\n",
    "<code>with open('mylist.pkl', 'rb') as f:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; loaded_list = pickle.load(f)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4jvsOqwMpQa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ftyx_HIB2Jf3"
   },
   "outputs": [],
   "source": [
    "# Extract Chunks using document paragraphs\n",
    "# Chunk size is controlled by parameter\n",
    "\n",
    "def extract_fixed_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Extract fixed-size chunks from a Word document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str or bytes): Path to Word document or binary content\n",
    "        chunk_size (int): Target size of each chunk in characters\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks of approximately chunk_size characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both file path and binary content\n",
    "        if isinstance(file_path, bytes):\n",
    "            doc = Document(BytesIO(file_path))\n",
    "        else:\n",
    "            doc = Document(file_path)\n",
    "\n",
    "        # Extract and clean all text\n",
    "        full_text = \"\"\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:  # Skip empty paragraphs\n",
    "                # Clean the text\n",
    "                text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "                full_text += text + \" \"  # Add space between paragraphs\n",
    "\n",
    "        # Split text into sentences\n",
    "        sentences = re.split('(?<=[.!?]) +', full_text)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If adding this sentence would exceed chunk_size\n",
    "            if len(current_chunk) + len(sentence) > chunk_size:\n",
    "                # If current chunk is not empty, add it to chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "\n",
    "                # Handle sentences longer than chunk_size\n",
    "                if len(sentence) > chunk_size:\n",
    "                    # Split long sentence into fixed-size chunks\n",
    "                    words = sentence.split()\n",
    "                    temp_chunk = \"\"\n",
    "\n",
    "                    for word in words:\n",
    "                        if len(temp_chunk) + len(word) + 1 <= chunk_size:\n",
    "                            temp_chunk += (\" \" + word if temp_chunk else word)\n",
    "                        else:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                            temp_chunk = word\n",
    "\n",
    "                    if temp_chunk:\n",
    "                        current_chunk = temp_chunk\n",
    "                else:\n",
    "                    current_chunk = sentence\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_chunk += (\" \" + sentence if current_chunk else sentence)\n",
    "\n",
    "        # Add the last chunk if not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing document: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fZdcpvLSHCy",
    "outputId": "8c4e1c8a-fc1f-44ff-bfca-d37602eebcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Word documents\n",
      "\n",
      "Processing: A Short History of Databases.docx\n",
      "Created 22 chunks\n",
      "\n",
      "Chunk 1 (length: 499):\n",
      "A Short History of Databases Databases have been integral to information storage and retrieval for centuries, evolving from simple library catalogs to sophisticated digital systems. Early databases were hierarchical and tree-oriented, but relational databases revolutionized data storage in the 1970s with structured query language (SQL). Over time, NoSQL databases emerged to accommodate unstructured data, and graph databases gained popularity due to their ability to handle complex relationships.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 474):\n",
      "The transition from traditional file-based storage to databases allowed for faster and more efficient data access. In the modern age, databases serve as the backbone of numerous applications, from enterprise systems to AI-driven technologies. However, their structure and philosophy differ significantly from ontologies, impacting their suitability for AI agent memory. Relational Database Construction Relational databases rely on structured tables with predefined schemas.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 448):\n",
      "The construction process involves: Normalization: Organizing data to reduce redundancy and improve consistency by structuring information into related tables. Schema Definition: Creating tables, defining attributes, setting constraints, and establishing relationships between tables to ensure data integrity. Query Optimization: Implementing indexing, caching, and optimization techniques to enhance performance and enable efficient data retrieval.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing: OverviewNNs.transformers.docx\n",
      "Created 95 chunks\n",
      "\n",
      "Chunk 1 (length: 355):\n",
      "Overview Neural networks have become a cornerstone in natural language processing (NLP), revolutionizing how machines understand and generate human language. In this chapter we’ll take a look at the structure of neural networks, how they work, how data is used to train a neural network and how neural networks have been applied to natural language tasks.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (length: 367):\n",
      "The Biology of the Brain In the brain, neurons are specialized cells that process and transmit information through a sophisticated system of electrical and chemical signals. Each neuron has dendrites that receive incoming signals, a cell body that processes them, and an axon that transmits outgoing signals to other neurons through connection points called synapses.\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (length: 492):\n",
      "When a neuron receives enough signals to reach a threshold, it \"fires,\" sending an electrical impulse that releases chemical messengers to influence connected neurons. Importantly, these neural connections can strengthen or weaken over time through a process called synaptic plasticity – the physical basis of learning and memory captured in the phrase \"neurons that fire together, wire together.” It's similar to how a path through a field becomes more defined the more people walk along it.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main - Note that chunk size to use is set here in main and overrides default\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Directory containing Word documents\n",
    "        directory = \"/content\"\n",
    "\n",
    "        # Get all .docx files in the directory\n",
    "        docx_files = list(Path(directory).glob(\"*.docx\"))\n",
    "\n",
    "        if not docx_files:\n",
    "            print(f\"No Word documents found in {directory}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(docx_files)} Word documents\")\n",
    "\n",
    "        # Process each document\n",
    "        for doc_path in docx_files:\n",
    "            try:\n",
    "                print(f\"\\nProcessing: {doc_path.name}\")\n",
    "\n",
    "                # Extract chunks of approximately 500 characters\n",
    "                # MODIFY this as you see fit\n",
    "                chunks = extract_fixed_chunks(str(doc_path), chunk_size=500)\n",
    "\n",
    "                print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "                # Print first few chunks with their lengths\n",
    "                for i, chunk in enumerate(chunks[:3], 1):\n",
    "                    print(f\"\\nChunk {i} (length: {len(chunk)}):\")\n",
    "                    print(chunk)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {doc_path.name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory: {str(e)}\")\n",
    "\n",
    "# Call main and start the chunking\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHBRrpl15FzG"
   },
   "source": [
    "/content/sample_data/mydata"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
